{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Under the Hood of ðŸ¬ SuiteEval\n",
    "\n",
    "Now let's look at the constituent structures of ðŸ¬ SuiteEval and how it can be extended to other benchmarks or tweaked.\n",
    "\n",
    "## Setup\n",
    "\n",
    "Set up a new environment if you're working locally! Otherwise you can just run all cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install git+https://github.com/Parry-Parry/suiteeval.git pyterrier-pisa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "\n",
    "import pandas as pd\n",
    "from pyterrier_pisa import PisaIndex\n",
    "\n",
    "from suiteeval import DatasetContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25(context: DatasetContext):\n",
    "    index = PisaIndex(\n",
    "        join(context.path, \"index.pisa\")\n",
    "    )  # context.path is a temporary directory\n",
    "    index.index(\n",
    "        context.get_corpus_iter()\n",
    "    )  # get_corpus_iter gives us a pyterrier compatible generator of {docno, text} records\n",
    "\n",
    "    return index.bm25(), \"BM25\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a New Suite\n",
    "\n",
    "If we want to make a new suite it can be as simple as registering the datasets and measures we want to use!\n",
    "\n",
    "Let's say we are primarily interested in multi-lingual IR, let's make a single suite that will run all of our datasets at once, we can do our cross-lingual and mono-lingual experiments at once. Let's add a broad set of measures to explore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    \"wikiclir/en-simple\",\n",
    "    \"wikiclir/es\",\n",
    "    \"wikiclir/fr\",\n",
    "    \"wikir/en59k/test\",\n",
    "    \"wikir/es13k/test\",\n",
    "    \"wikir/fr14k/test\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ir_measures import nDCG, P, R\n",
    "\n",
    "from suiteeval import Suite\n",
    "\n",
    "MLIR = Suite.register(\n",
    "    \"MultiLingualIR\",\n",
    "    datasets=datasets,\n",
    "    metadata={\n",
    "        \"description\": \"A multilingual information retrieval benchmark combining WikiCLIR and WikiR datasets.\",\n",
    "        \"official_measures\": [nDCG @ 10, P @ 10, R @ 1000],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! We can now run our new suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLIR(bm25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Fancy\n",
    "\n",
    "Let's now consider that we want to see overall performance of our models / baselines on multi-lingual versus cross-lingual IR, here we can subclass Suite to implement this behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from suiteeval.utility import geometric_mean\n",
    "\n",
    "\n",
    "class _MLIR(Suite):\n",
    "    def compute_overall_mean(\n",
    "        self, results: pd.DataFrame, eval_metrics=None\n",
    "    ) -> pd.DataFrame:\n",
    "        measure_cols = [\n",
    "            str(m)\n",
    "            for m in (eval_metrics or self.__default_measures)\n",
    "            if str(m) in results.columns\n",
    "        ]\n",
    "        if not measure_cols:\n",
    "            return results\n",
    "\n",
    "        # 1) Per-(dataset, name) geometric means (no relabel yet)\n",
    "        gmean_rows = []\n",
    "        for (dataset, name), group in results.groupby(\n",
    "            [\"dataset\", \"name\"], dropna=False\n",
    "        ):\n",
    "            row = {\"dataset\": dataset, \"name\": name}\n",
    "            for col in measure_cols:\n",
    "                vals = pd.to_numeric(group[col], errors=\"coerce\").dropna().values\n",
    "                if vals.size:\n",
    "                    row[col] = geometric_mean(vals)\n",
    "            gmean_rows.append(row)\n",
    "        per_ds_df = pd.DataFrame(gmean_rows)\n",
    "\n",
    "        # 2) Multi-Lingual and Cross-Lingual overalls computed from per-dataset means\n",
    "        def _overall_for_prefix(prefix: str, label: str) -> pd.DataFrame:\n",
    "            subset = per_ds_df[\n",
    "                per_ds_df[\"dataset\"].astype(str).str.startswith(prefix, na=False)\n",
    "            ]\n",
    "            if subset.empty:\n",
    "                return pd.DataFrame(columns=[\"dataset\", \"name\"] + measure_cols)\n",
    "            rows = []\n",
    "            for name, grp in subset.groupby(\"name\", dropna=False):\n",
    "                row = {\"dataset\": label, \"name\": name}\n",
    "                for col in measure_cols:\n",
    "                    vals = pd.to_numeric(grp[col], errors=\"coerce\").dropna().values\n",
    "                    if vals.size:\n",
    "                        row[col] = geometric_mean(vals)\n",
    "                rows.append(row)\n",
    "            return pd.DataFrame(rows)\n",
    "\n",
    "        mlir_df = _overall_for_prefix(\"wikir\", \"Overall (Multi-Lingual)\")\n",
    "        clir_df = _overall_for_prefix(\"wikiclir\", \"Overall (Cross-Lingual)\")\n",
    "\n",
    "        # 3) Preserve your existing \"Overall\" behaviour (relabel per-dataset means)\n",
    "        gmean_df = per_ds_df.copy()\n",
    "        gmean_df[\"dataset\"] = \"Overall\"\n",
    "\n",
    "        # 4) Concatenate and return\n",
    "        return pd.concat([results, gmean_df, mlir_df, clir_df], ignore_index=True)\n",
    "\n",
    "\n",
    "MLIR = _MLIR()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now not only do we run all of our evaluation but we get principled averages of our different tasks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLIR(bm25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try yourself!\n",
    "\n",
    "Group your favourite datasets or head over to [ir-datasets](ir-datasets.com) and choose a custom collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ir_measures import *\n",
    "\n",
    "datasets = []\n",
    "measures = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MySuite = Suite.register(\n",
    "    \"MySuite\",\n",
    "    datasets=datasets,\n",
    "    metadata={\n",
    "        \"description\": \"My custom IR benchmark suite.\",\n",
    "        \"official_measures\": measures,\n",
    "    },\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
